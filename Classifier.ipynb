{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One shot learning using triplet loss\n",
    "\n",
    "The main idea of this project is to experiment with keras and the omniglot dataset to build a *\"One shot learning\"* model.\n",
    "\n",
    "These kind of models solves categorization problem by seeing only a few exemples of each catgories.\n",
    "\n",
    "The dataset used is the *omniglot* dataset. It contains 4840 handwritten letters from differents alphabet with 20 examples for each letters. You can find the dataset here: https://github.com/brendenlake/omniglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity, pairwise_distances\n",
    "\n",
    "from keras import applications\n",
    "from keras import callbacks\n",
    "from keras import layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras import backend as K\n",
    "from keras.utils import Sequence\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The triplet loss approach\n",
    "\n",
    "To learn more about the triplet loss you can read this : https://towardsdatascience.com/siamese-network-triplet-loss-b4ca82c1aec8\n",
    "\n",
    "Here is the article talking about the triplet loss function used in my example : https://towardsdatascience.com/lossless-triplet-loss-7e932f990b24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss functions\n",
    "\n",
    "def triplet_loss(y_true, y_pred, N = 126, beta = 126, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    Implementation of the triplet loss function\n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- true labels, required when you define a loss in Keras, you don't need it in this function.\n",
    "    y_pred -- python list containing three objects:\n",
    "            anchor -- the encodings for the anchor data\n",
    "            positive -- the encodings for the positive data (similar to anchor)\n",
    "            negative -- the encodings for the negative data (different from anchor)\n",
    "    N  --  The number of dimension \n",
    "    beta -- The scaling factor, N is recommended\n",
    "    epsilon -- The Epsilon value to prevent ln(0)\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    loss -- real number, value of the loss\n",
    "    \"\"\"\n",
    "    anchor = tf.convert_to_tensor(y_pred[:,0:N])\n",
    "    positive = tf.convert_to_tensor(y_pred[:,N:N*2]) \n",
    "    negative = tf.convert_to_tensor(y_pred[:,N*2:N*3])\n",
    "    \n",
    "    # distance between the anchor and the positive\n",
    "    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,positive)),1)\n",
    "    # distance between the anchor and the negative\n",
    "    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor,negative)),1)\n",
    "    \n",
    "    #Non Linear Values  \n",
    "    \n",
    "    # -ln(-x/N+1)\n",
    "    pos_dist = -tf.log(-tf.divide((pos_dist),beta)+1+epsilon)\n",
    "    neg_dist = -tf.log(-tf.divide((N-neg_dist),beta)+1+epsilon)\n",
    "    \n",
    "    # compute loss\n",
    "    loss = neg_dist + pos_dist\n",
    "    \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Network\n",
    "\n",
    "The \"encoding\" network is a classical CNN. Once trained he will map the images in a latent space where all the similar letters will be clusterised.\n",
    "\n",
    "For the structure of the training model I send you again to this article : https://towardsdatascience.com/siamese-network-triplet-loss-b4ca82c1aec8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_embedding_network(input_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(layers.Conv2D(32, kernel_size=(3,3),\n",
    "                    activation='relu',\n",
    "                    input_shape=input_shape))\n",
    "\n",
    "    model.add(layers.Conv2D(64, (3,3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D(pool_size=2))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(126, activation='sigmoid'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def get_training_model(input_shape):\n",
    "    \n",
    "    anchor_input = layers.Input(shape=input_shape)\n",
    "    positive_input = layers.Input(shape=input_shape)\n",
    "    negative_input = layers.Input(shape=input_shape)\n",
    "\n",
    "    embedding_net = get_embedding_network(input_shape)\n",
    "\n",
    "    anchor_embedding = embedding_net(anchor_input)\n",
    "    positive_embedding = embedding_net(positive_input)\n",
    "    negative_embedding = embedding_net(negative_input)\n",
    "\n",
    "    merged = layers.concatenate([anchor_embedding, positive_embedding, negative_embedding], axis=-1)\n",
    "\n",
    "    model = Model(inputs=[anchor_input, positive_input, negative_input], outputs=merged)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the omniglot dataset\n",
    "\n",
    "The boring stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset function utils\n",
    "\n",
    "def load_dataframe(languages):\n",
    "    \"\"\"\n",
    "        This function take an array of language name and\n",
    "        index all the symbol of each language in a dataframe\n",
    "        with column : id, path\n",
    "    \"\"\"\n",
    "    \n",
    "    sign_id = []\n",
    "    sign_path = []\n",
    "    \n",
    "    for language in languages:\n",
    "        ids, paths = scan_directory(language)\n",
    "        \n",
    "        sign_id += ids\n",
    "        sign_path += paths\n",
    "        \n",
    "    file = [('id', sign_id),\n",
    "             ('path', sign_path),\n",
    "           ]\n",
    "    \n",
    "    return pd.DataFrame.from_items(file)\n",
    "\n",
    "\n",
    "def scan_directory(directory_name):\n",
    "    path = Path(\"./omniglot/{}\".format(directory_name))\n",
    "    \n",
    "    sign_id = []\n",
    "    sign_path = []\n",
    "    \n",
    "    for elem in path.iterdir():\n",
    "        if elem.is_dir():\n",
    "            ids, paths = process_char(elem)\n",
    "            \n",
    "            sign_id += ids\n",
    "            sign_path += paths\n",
    "            \n",
    "    return sign_id, sign_path\n",
    "            \n",
    "\n",
    "def process_char(char_path):\n",
    "    \n",
    "    sign_id = []\n",
    "    file_path = []\n",
    "    \n",
    "    for elem in char_path.iterdir():\n",
    "        if elem.is_dir():\n",
    "            continue\n",
    "            \n",
    "        # Processing the file\n",
    "        path_str = str(elem)\n",
    "        \n",
    "        file_name = path_str.split(\"/\")[-1]\n",
    "        img_id = file_name.split(\"_\")[0]\n",
    "        \n",
    "        sign_id.append(img_id)\n",
    "        file_path.append(path_str)\n",
    "    \n",
    "    return sign_id, file_path\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    pic = Image.open(image_path)\n",
    "    img = np.true_divide(np.array(pic), 255)\n",
    "    return img[:, :, np.newaxis]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split test and train\n",
    "\n",
    "I decided to train my model on the letter from 6 alphabets and to test it on 2 other alphabet. Therefore I'm sure that my model has never saw the test letters before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'omniglot/Balinese'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e53b31197f8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Loading train and data and printing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_languages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_languages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d9f77e38348c>\u001b[0m in \u001b[0;36mload_dataframe\u001b[0;34m(languages)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlanguages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscan_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0msign_id\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d9f77e38348c>\u001b[0m in \u001b[0;36mscan_directory\u001b[0;34m(directory_name)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0msign_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0melem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_char\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/lib/python3.7/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1102\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'omniglot/Balinese'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_languages = [\"Balinese\", \"Early_Aramaic\", \"Greek\", \"Japanese_(katakana)\", \"Korean\", \"Latin\"]\n",
    "test_languages = [\"Sanskrit\", \"Tagalog\"]\n",
    "\n",
    "# Loading train and data and printing\n",
    "train_set = load_dataframe(train_languages)\n",
    "test_set = load_dataframe(test_languages)\n",
    "\n",
    "path = train_set[\"path\"].iloc[1234]\n",
    "\n",
    "TEST_IMG = load_image(path)\n",
    "disp_img = TEST_IMG.reshape(TEST_IMG.shape[:2])\n",
    "plt.imshow(disp_img, cmap=\"gray\")\n",
    "\n",
    "print(\"train len {}\".format(len(train_set)))\n",
    "print(\"test len : {}\".format(len(test_set)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Triplet Generator\n",
    "\n",
    "In order to feed our model with images and to avoid to explode our memory we are going to generate images on the fly (https://keras.io/utils/#sequence)\n",
    "\n",
    "This generator will compose random triplet when asked for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TripletGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, batch_size, nbr_triplet, data):\n",
    "        \n",
    "        self.data = data\n",
    "        self.nbr_triplet = nbr_triplet\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.nbr_triplet / self.batch_size).astype(np.int64)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # generate random triplet\n",
    "        \n",
    "        anchors = []\n",
    "        negs = []\n",
    "        pos = []\n",
    "        \n",
    "        selected_char = self.data.sample(n=self.batch_size)\n",
    "        \n",
    "        \n",
    "        for index, row in selected_char.iterrows():\n",
    "            \n",
    "            anchors.append(load_image(row[\"path\"]))\n",
    "            \n",
    "            negs.append(self.get_neg(row[\"id\"]))\n",
    "            pos.append(self.get_pos(row[\"id\"], row[\"path\"]))\n",
    "            \n",
    "        X = [np.array(anchors), np.array(pos), np.array(negs)]\n",
    "        y = np.ones(len(anchors))\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def get_pos(self, id, path):\n",
    "        same = self.data[self.data[\"id\"] == id]\n",
    "        same = same[same[\"path\"] != path]\n",
    "        sel = same.sample(n=1)\n",
    "        return load_image(sel[\"path\"].iloc[0])\n",
    "        \n",
    "        \n",
    "    def get_neg(self, id):\n",
    "        diff = self.data[self.data[\"id\"] != id]\n",
    "        sel = diff.sample(n=1)\n",
    "        return load_image(sel[\"path\"].iloc[0])\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the triplet loss model\n",
    "\n",
    "The training is made on 100 epoch with a batch size of 1000 triplets and 10 000 triplets generated per epoch.\n",
    "\n",
    "Due to CPU restriction, I wasn't able to run it for much longer but we can see that the loss slowly decrease during the 100 epoch. \n",
    "\n",
    "(And due to my stupidity, I erased the result so I will rerun this cell one day to show you the drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jfink/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1264: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/jfink/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 105, 105, 1)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 126)          20515198    input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 378)          0           sequential_1[1][0]               \n",
      "                                                                 sequential_1[2][0]               \n",
      "                                                                 sequential_1[3][0]               \n",
      "==================================================================================================\n",
      "Total params: 20,515,198\n",
      "Trainable params: 20,515,198\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      " 3/10 [========>.....................] - ETA: 19s - loss: nan"
     ]
    }
   ],
   "source": [
    "train_model = get_training_model(TEST_IMG.shape)\n",
    "train_model.compile(loss=triplet_loss, optimizer=Adam())\n",
    "\n",
    "train_model.summary()\n",
    "\n",
    "gen_train = TripletGenerator(10, 100, train_set)\n",
    "\n",
    "train_model.fit_generator(generator=gen_train, verbose=1, use_multiprocessing=True, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test embeddings TSNE\n",
    "\n",
    "We are going to map 10 char in the latent space to see if they are far from each other in the latent space.\n",
    "\n",
    "We can clearly see some clusters but 100 epoch don't seem to be enough to have nice clusters for each letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extracting embedding model\n",
    "\n",
    "seq_input = train_model.layers[3].get_input_at(-1)\n",
    "\n",
    "encoder = Model(inputs = seq_input,\n",
    "                outputs=train_model.layers[3].get_output_at(-1))\n",
    "\n",
    "\n",
    "\n",
    "# Selecting 10 test char\n",
    "sample = random.sample(test_set[\"id\"].tolist(), 10)\n",
    "\n",
    "X = []\n",
    "label = []\n",
    "\n",
    "for id_img in sample:\n",
    "    \n",
    "    cur_df = test_set[test_set[\"id\"] == id_img]\n",
    "    \n",
    "    for index, row in cur_df.head(10).iterrows():\n",
    "\n",
    "        img = load_image(row[\"path\"])\n",
    " \n",
    "        embed = encoder.predict(np.array([np.array(img)]))\n",
    "        \n",
    "        X.append(embed)\n",
    "        label.append(id_img)\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "label = np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEyCAYAAAD5tWvWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f541d7b68d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "nsamples, nx, ny = X.shape\n",
    "X = X.reshape((nsamples,nx*ny))\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "X_2d = tsne.fit_transform(X)\n",
    "\n",
    "target_name = list(set(label.tolist()))\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(6, 5))\n",
    "colors = ['r', 'g', 'b', 'c', 'm', 'y', 'k', 'w', 'orange', 'purple']\n",
    "dic_col = {}\n",
    "\n",
    "print(len(target_name))\n",
    "\n",
    "for i in range(len(target_name)):\n",
    "    dic_col[target_name[i]] = colors[i]\n",
    "\n",
    "for index_data in range(len(X_2d)):\n",
    "    plt.scatter(X_2d[index_data, 0], X_2d[index_data, 1], c=dic_col[label[index_data]])\n",
    "\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the model on new data\n",
    "\n",
    "Generating duet test data and feeding them in the network to test it.\n",
    "\n",
    "The evaluation process consist to encode two images in our latent space and to predict if they are the same or different (decision problem). To evaluate the similarity between two point we use the **cosine similarity** metrics. Not that no other metrics where tryed maybe could you come with something better.\n",
    "\n",
    "for the 1000 duet prediction the model was right for 614 (61,4% accurracy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuetGenerator(Sequence):\n",
    "    def __init__(self, batch_size, nbr_double, data, taux_positif):\n",
    "        \n",
    "        self.data = data\n",
    "        self.nbr_double = nbr_double\n",
    "        self.taux_positif = taux_positif\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        #Estimate len\n",
    "        return np.ceil(self.nbr_double / self.batch_size).astype(np.int64)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_1 = []\n",
    "        img_2 = []\n",
    "        y = []\n",
    "\n",
    "        selected = self.data.sample(n=self.batch_size)\n",
    "\n",
    "        for index, row in selected.iterrows():\n",
    "            \n",
    "            r = random.random()\n",
    "\n",
    "            img_1.append(load_image(row[\"path\"]))\n",
    "\n",
    "            if(r < self.taux_positif):\n",
    "                img_2.append(self.get_pos(row[\"id\"], row[\"path\"]))\n",
    "                y.append(1)\n",
    "            else:\n",
    "                img_2.append(self.get_neg(row[\"id\"]))\n",
    "                y.append(0)\n",
    "            \n",
    "        X = [np.array(img_1), np.array(img_2)]\n",
    "\n",
    "        return X, np.array(y)\n",
    "\n",
    "    def get_pos(self, id, path):\n",
    "        same = self.data[self.data[\"id\"] == id]\n",
    "        same = same[same[\"path\"] != path]\n",
    "        sel = same.sample(n=1)\n",
    "        return load_image(sel[\"path\"].iloc[0])\n",
    "        \n",
    "        \n",
    "    def get_neg(self, id):\n",
    "        diff = self.data[self.data[\"id\"] != id]\n",
    "        sel = diff.sample(n=1)\n",
    "        return load_image(sel[\"path\"].iloc[0])\n",
    "\n",
    "    \n",
    "def predict(model, img_1, img_2):\n",
    "    emb_1 = model.predict(np.array([np.array(img_1)]))\n",
    "    emb_2 = model.predict(np.array([np.array(img_2)]))\n",
    "\n",
    "    return pairwise_distances(emb_1, emb_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ----- RESULTS ----\n",
      "614 / 1000 prediction correcte (accuracy : 0.614 )\n",
      "\n",
      "Taux de faux positif : 0.144\n",
      "\n",
      "Taux de faux negatif : 0.242\n",
      "\n",
      "\n",
      "Moyenne des prédictions pour classe pos : 0.06289879840313772\n",
      "\n",
      "Moyenne des prédictions pour classe neg : 0.07330304373763213\n"
     ]
    }
   ],
   "source": [
    "get_test = DuetGenerator(1, 1000, test_set, 0.5)\n",
    "\n",
    "nbr_test = 0\n",
    "nbr_success = 0\n",
    "\n",
    "faux_pos = 0 \n",
    "faux_neg = 0\n",
    "\n",
    "pos_val = []\n",
    "neg_val = []\n",
    "\n",
    "for i in range(0,get_test.__len__()):\n",
    "\n",
    "    data, y = get_test.__getitem__(1)\n",
    "\n",
    "    pred_full = predict(encoder, data[0][0], data[1][0])[0]\n",
    "    pred = pred_full[0]\n",
    "    #print(\"Predict:  {} expected: {}\\n\".format(pred, y[0]))\n",
    "\n",
    "    nbr_test += 1\n",
    "\n",
    "    if pred < 0.061:\n",
    "        rounded_pred = 1\n",
    "    else:\n",
    "        rounded_pred = 0\n",
    "\n",
    "    if rounded_pred == y[0]:\n",
    "        nbr_success += 1\n",
    "    \n",
    "    elif rounded_pred == 0:\n",
    "        faux_neg += 1\n",
    "\n",
    "    else:\n",
    "        faux_pos += 1\n",
    "\n",
    "    if y[0]:\n",
    "        pos_val.append(pred)\n",
    "    else:\n",
    "        neg_val.append(pred)\n",
    "    \n",
    "\n",
    "    \n",
    "print(\"\\n\\n ----- RESULTS ----\")\n",
    "print(\"{} / {} prediction correcte (accuracy : {} )\".format(\n",
    "    nbr_success, \n",
    "    nbr_test, \n",
    "    (nbr_success/nbr_test)))\n",
    "\n",
    "print(\"\\nTaux de faux positif : {}\".format((faux_pos/nbr_test)))\n",
    "print(\"\\nTaux de faux negatif : {}\".format((faux_neg/nbr_test)))\n",
    "\n",
    "print(\"\\n\\nMoyenne des prédictions pour classe pos : {}\".format(sum(pos_val)/len(pos_val)))\n",
    "print(\"\\nMoyenne des prédictions pour classe neg : {}\".format(sum(neg_val)/len(neg_val)))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
